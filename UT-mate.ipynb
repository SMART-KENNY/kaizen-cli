{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f177b7b5-ff8a-4a63-b2df-a598da66f93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n  Obtaining dependency information for mlflow from https://files.pythonhosted.org/packages/52/fe/1ed27f800cd1709a272c6e26b78ec3d77a5ba482171ea1b5bfbcf4c067c0/mlflow-3.4.0-py3-none-any.whl.metadata\n  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: databricks-sdk[openai] in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (0.65.0)\nCollecting mlflow-skinny==3.4.0 (from mlflow)\n  Obtaining dependency information for mlflow-skinny==3.4.0 from https://files.pythonhosted.org/packages/1b/94/7acd7c6970cc75da1fd3b550e43d8b99068032022f47b0ef224a137ec679/mlflow_skinny-3.4.0-py3-none-any.whl.metadata\n  Downloading mlflow_skinny-3.4.0-py3-none-any.whl.metadata (31 kB)\nCollecting mlflow-tracing==3.4.0 (from mlflow)\n  Obtaining dependency information for mlflow-tracing==3.4.0 from https://files.pythonhosted.org/packages/ae/96/403b1191ccf587f19a8c94085477600d6e6b3d61a7aff46f353b20b450f9/mlflow_tracing-3.4.0-py3-none-any.whl.metadata\n  Downloading mlflow_tracing-3.4.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow) (3.1.2)\nCollecting alembic!=1.10.0,<2 (from mlflow)\n  Obtaining dependency information for alembic!=1.10.0,<2 from https://files.pythonhosted.org/packages/39/4a/4c61d4c84cfd9befb6fa08a702535b27b21fff08c946bc2f6139decbf7f7/alembic-1.16.5-py3-none-any.whl.metadata\n  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\nCollecting cryptography<46,>=43.0.0 (from mlflow)\n  Obtaining dependency information for cryptography<46,>=43.0.0 from https://files.pythonhosted.org/packages/0e/e4/b3e68a4ac363406a56cf7b741eeb80d05284d8c60ee1a55cdc7587e2a553/cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl.metadata\n  Downloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow) (7.1.0)\nCollecting fastmcp<3,>=2.0.0 (from mlflow)\n  Obtaining dependency information for fastmcp<3,>=2.0.0 from https://files.pythonhosted.org/packages/96/79/0fd386e61819e205563d4eb15da76564b80dc2edd3c64b46f2706235daec/fastmcp-2.12.3-py3-none-any.whl.metadata\n  Downloading fastmcp-2.12.3-py3-none-any.whl.metadata (17 kB)\nCollecting graphene<4 (from mlflow)\n  Obtaining dependency information for graphene<4 from https://files.pythonhosted.org/packages/66/e0/61d8e98007182e6b2aca7cf65904721fb2e4bce0192272ab9cb6f69d8812/graphene-3.4.3-py2.py3-none-any.whl.metadata\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Obtaining dependency information for gunicorn<24 from https://files.pythonhosted.org/packages/cb/7d/6dac2a6e1eba33ee43f318edbed4ff29151a49b5d37f080aad1e6469bca4/gunicorn-23.0.0-py3-none-any.whl.metadata\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (3.7.2)\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.23.5)\nRequirement already satisfied: pandas<3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow) (2.3.2)\nRequirement already satisfied: pyarrow<22,>=4.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow) (21.0.0)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.3.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.11.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow) (2.0.43)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (8.3.0)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (2.2.1)\nRequirement already satisfied: fastapi<1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.117.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (6.0.0)\nCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for opentelemetry-api<3,>=1.9.0 from https://files.pythonhosted.org/packages/91/48/28ed9e55dcf2f453128df738210a980e09f4e468a456fa3c763dbc8be70a/opentelemetry_api-1.37.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for opentelemetry-proto<3,>=1.9.0 from https://files.pythonhosted.org/packages/c4/25/f89ea66c59bd7687e218361826c969443c4fa15dfe89733f3bf1e2a9e971/opentelemetry_proto-1.37.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for opentelemetry-sdk<3,>=1.9.0 from https://files.pythonhosted.org/packages/9f/62/9f4ad6a54126fb00f7ed4bb5034964c6e4f00fcd5a905e115bd22707e20d/opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata\n  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (23.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (4.24.1)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (2.11.9)\nRequirement already satisfied: python-dotenv<2,>=0.19.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (1.1.1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (6.0)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (2.31.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.5.0)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mlflow-skinny==3.4.0->mlflow) (4.15.0)\nCollecting uvicorn<1 (from mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for uvicorn<1 from https://files.pythonhosted.org/packages/96/06/5cc0542b47c0338c1cb676b348e24a1c29acabc81000bced518231dded6f/uvicorn-0.36.0-py3-none-any.whl.metadata\n  Downloading uvicorn-0.36.0-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk[openai]) (2.31.0)\nCollecting openai (from databricks-sdk[openai])\n  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/38/87/6ad18ce0e7b910e3706480451df48ff9e0af3b55e5db565adafd68a0706a/openai-1.108.1-py3-none-any.whl.metadata\n  Downloading openai-1.108.1-py3-none-any.whl.metadata (29 kB)\nCollecting langchain-openai (from databricks-sdk[openai])\n  Obtaining dependency information for langchain-openai from https://files.pythonhosted.org/packages/67/31/af0486b7ad8a49f3c5c852ca2b3a7f6d8526cc71a405045dd959c36ec5db/langchain_openai-0.3.33-py3-none-any.whl.metadata\n  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\nCollecting httpx (from databricks-sdk[openai])\n  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow)\n  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/87/fb/99f81ac72ae23375f22b7afdb7642aba97c00a713c217124420147681a2f/mako-1.3.10-py3-none-any.whl.metadata\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: cffi>=1.14 in /databricks/python3/lib/python3.11/site-packages (from cryptography<46,>=43.0.0->mlflow) (1.15.1)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.11/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.16)\nCollecting authlib>=1.5.2 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for authlib>=1.5.2 from https://files.pythonhosted.org/packages/0e/aa/91355b5f539caf1b94f0e66ff1e4ee39373b757fce08204981f7829ede51/authlib-1.6.4-py2.py3-none-any.whl.metadata\n  Downloading authlib-1.6.4-py2.py3-none-any.whl.metadata (9.8 kB)\nCollecting cyclopts>=3.0.0 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for cyclopts>=3.0.0 from https://files.pythonhosted.org/packages/f0/8b/2c95f0645c6f40211896375e6fa51f504b8ccb29c21f6ae661fe87ab044e/cyclopts-3.24.0-py3-none-any.whl.metadata\n  Downloading cyclopts-3.24.0-py3-none-any.whl.metadata (11 kB)\nCollecting exceptiongroup>=1.2.2 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for exceptiongroup>=1.2.2 from https://files.pythonhosted.org/packages/36/f4/c6e662dade71f56cd2f3735141b265c3c79293c109549c1e6933b0651ffc/exceptiongroup-1.3.0-py3-none-any.whl.metadata\n  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\nCollecting mcp<2.0.0,>=1.12.4 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for mcp<2.0.0,>=1.12.4 from https://files.pythonhosted.org/packages/8e/11/d334fbb7c2aeddd2e762b86d7a619acffae012643a5738e698f975a2a9e2/mcp-1.14.1-py3-none-any.whl.metadata\n  Downloading mcp-1.14.1-py3-none-any.whl.metadata (75 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/75.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.5/75.5 kB\u001B[0m \u001B[31m19.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting openapi-core>=0.19.5 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for openapi-core>=0.19.5 from https://files.pythonhosted.org/packages/27/6f/83ead0e2e30a90445ee4fc0135f43741aebc30cca5b43f20968b603e30b6/openapi_core-0.19.5-py3-none-any.whl.metadata\n  Downloading openapi_core-0.19.5-py3-none-any.whl.metadata (6.6 kB)\nCollecting openapi-pydantic>=0.5.1 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for openapi-pydantic>=0.5.1 from https://files.pythonhosted.org/packages/12/cf/03675d8bd8ecbf4445504d8071adab19f5f993676795708e36402ab38263/openapi_pydantic-0.5.1-py3-none-any.whl.metadata\n  Downloading openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\nCollecting pyperclip>=1.9.0 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for pyperclip>=1.9.0 from https://files.pythonhosted.org/packages/1e/bc/22540e73c5f5ae18f02924cd3954a6c9a4aa6b713c841a94c98335d333a1/pyperclip-1.10.0-py3-none-any.whl.metadata\n  Downloading pyperclip-1.10.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting rich>=13.9.4 (from fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for rich>=13.9.4 from https://files.pythonhosted.org/packages/e3/30/3c4d035596d3cf444529e0b2953ad0466f6049528a879d27534700580395/rich-14.1.0-py3-none-any.whl.metadata\n  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: blinker>=1.9.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: jinja2>=3.1.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.6)\nRequirement already satisfied: markupsafe>=2.1.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from Flask<4->mlflow) (3.0.2)\nRequirement already satisfied: werkzeug>=3.1.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk[openai]) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk[openai]) (4.9)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.6)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Obtaining dependency information for graphql-relay<3.3,>=3.1 from https://files.pythonhosted.org/packages/74/16/a4cf06adbc711bd364a73ce043b0b08d8fa5aae3df11b6ee4248bcdad2e0/graphql_relay-3.2.0-py3-none-any.whl.metadata\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.11/site-packages (from graphene<4->mlflow) (2.8.2)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from httpx->databricks-sdk[openai]) (4.10.0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.11/site-packages (from httpx->databricks-sdk[openai]) (2023.7.22)\nCollecting httpcore==1.* (from httpx->databricks-sdk[openai])\n  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.11/site-packages (from httpx->databricks-sdk[openai]) (3.4)\nCollecting h11>=0.16 (from httpcore==1.*->httpx->databricks-sdk[openai])\n  Obtaining dependency information for h11>=0.16 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (9.4.0)\nCollecting pyparsing<3.1,>=2.3.1 (from matplotlib<4->mlflow)\n  Obtaining dependency information for pyparsing<3.1,>=2.3.1 from https://files.pythonhosted.org/packages/6c/10/a7d0fa5baea8fe7b50f448ab742f26f52b80bfca85ac2be9d35cdd9a3246/pyparsing-3.0.9-py3-none-any.whl.metadata\n  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<3->mlflow) (2022.7)\nRequirement already satisfied: tzdata>=2022.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pandas<3->mlflow) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow) (2.0.4)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\nCollecting langchain-core<1.0.0,>=0.3.76 (from langchain-openai->databricks-sdk[openai])\n  Obtaining dependency information for langchain-core<1.0.0,>=0.3.76 from https://files.pythonhosted.org/packages/77/b5/501c0ffcb09c734457ceaa86bc7b1dd37b6a261147bd653add03b838aacb/langchain_core-0.3.76-py3-none-any.whl.metadata\n  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\nCollecting tiktoken<1,>=0.7 (from langchain-openai->databricks-sdk[openai])\n  Obtaining dependency information for tiktoken<1,>=0.7 from https://files.pythonhosted.org/packages/34/9a/db7a86b829e05a01fd4daa492086f708e0a8b53952e1dbc9d380d2b03677/tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->databricks-sdk[openai]) (1.7.0)\nCollecting jiter<1,>=0.4.0 (from openai->databricks-sdk[openai])\n  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/9f/d8/ec74886497ea393c29dbd7651ddecc1899e86404a6b1f84a3ddab0ab59fd/jiter-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading jiter-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: sniffio in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openai->databricks-sdk[openai]) (1.3.1)\nRequirement already satisfied: tqdm>4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openai->databricks-sdk[openai]) (4.67.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.14->cryptography<46,>=43.0.0->mlflow) (2.21)\nRequirement already satisfied: attrs>=23.1.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (25.3.0)\nCollecting docstring-parser>=0.15 (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for docstring-parser>=0.15 from https://files.pythonhosted.org/packages/55/e2/2537ebcff11c1ee1ff17d8d0b6f4db75873e3b0fb32c2d4a2ee31ecb310a/docstring_parser-0.17.0-py3-none-any.whl.metadata\n  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for rich-rst<2.0.0,>=1.3.1 from https://files.pythonhosted.org/packages/fd/bc/cc4e3dbc5e7992398dcb7a8eda0cbcf4fb792a0cdb93f857b478bf3cf884/rich_rst-1.3.1-py3-none-any.whl.metadata\n  Downloading rich_rst-1.3.1-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: starlette<0.49.0,>=0.40.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from fastapi<1->mlflow-skinny==3.4.0->mlflow) (0.48.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.4.0->mlflow) (3.11.0)\nCollecting langsmith>=0.3.45 (from langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai])\n  Obtaining dependency information for langsmith>=0.3.45 from https://files.pythonhosted.org/packages/f2/a5/56169ce49b3020b47112703b2f9ed0e3255073c8d438b74406b290fb5687/langsmith-0.4.29-py3-none-any.whl.metadata\n  Downloading langsmith-0.4.29-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]) (8.2.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]) (1.33)\nCollecting httpx-sse>=0.4 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for httpx-sse>=0.4 from https://files.pythonhosted.org/packages/25/0a/6269e3473b09aed2dab8aa1a600c70f31f00ae1349bee30658f7e358a159/httpx_sse-0.4.1-py3-none-any.whl.metadata\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: jsonschema>=4.20.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (4.25.1)\nCollecting pydantic-settings>=2.5.2 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for pydantic-settings>=2.5.2 from https://files.pythonhosted.org/packages/58/f0/427018098906416f580e3cf1366d3b1abfb408a0652e9f31600c24a1903c/pydantic_settings-2.10.1-py3-none-any.whl.metadata\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for python-multipart>=0.0.9 from https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl.metadata\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for sse-starlette>=1.6.1 from https://files.pythonhosted.org/packages/ef/10/c78f463b4ef22eef8491f218f692be838282cd65480f6e423d7730dfd1fb/sse_starlette-3.0.2-py3-none-any.whl.metadata\n  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: isodate in /databricks/python3/lib/python3.11/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.6.1)\nRequirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.3.4)\nRequirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (8.10.0)\nRequirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.6.3)\nRequirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.7.2)\nCollecting parse (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for parse from https://files.pythonhosted.org/packages/d0/31/ba45bf0b2aa7898d81cbbfac0e88c267befb59ad91a19e36e1bc5578ddb1/parse-1.20.2-py2.py3-none-any.whl.metadata\n  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n  Obtaining dependency information for werkzeug>=3.1.0 from https://files.pythonhosted.org/packages/ee/ea/c67e1dee1ba208ed22c06d1d547ae5e293374bfc43e0eb0ef5e262b68561/werkzeug-3.1.1-py3-none-any.whl.metadata\n  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for protobuf<7,>=3.12.0 from https://files.pythonhosted.org/packages/5c/f6/88d77011b605ef979aace37b7703e4eefad066f7e84d935e5a696515c2dd/protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata\n  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\nCollecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for opentelemetry-semantic-conventions==0.58b0 from https://files.pythonhosted.org/packages/07/90/68152b7465f50285d3ce2481b3aec2f82822e3f52e5152eeeaf516bab841/opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata\n  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk[openai]) (0.4.8)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.4.1)\nCollecting email-validator>=2.0.0 (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for email-validator>=2.0.0 from https://files.pythonhosted.org/packages/de/15/545e2b6cf2e3be84bc1ed85613edd75b8aea69807a71c26f4ca6a9258e82/email_validator-2.3.0-py3-none-any.whl.metadata\n  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\nCollecting markdown-it-py>=2.2.0 (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/94/54/e7d793b573f298e1c9013b8c4dade17d481164aa517d1d7148619c2cedbf/markdown_it_py-4.0.0-py3-none-any.whl.metadata\n  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (2.19.2)\nRequirement already satisfied: regex>=2022.1.18 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai->databricks-sdk[openai]) (2025.9.18)\nCollecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow)\n  Obtaining dependency information for dnspython>=2.0.0 from https://files.pythonhosted.org/packages/ba/5a/18ad964b0086c6e62e2e7500f7edc89e3faa45033c71c1893d34eed2b2de/dnspython-2.8.0-py3-none-any.whl.metadata\n  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (5.0.1)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai]) (3.0.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.27.1)\nRequirement already satisfied: pathable<0.5.0,>=0.4.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.4.4)\nCollecting orjson>=3.9.14 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai])\n  Obtaining dependency information for orjson>=3.9.14 from https://files.pythonhosted.org/packages/bb/6a/e5bf7b70883f374710ad74faf99bacfc4b5b5a7797c1d5e130350e0e28a3/orjson-3.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading orjson-3.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/41.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.9/41.9 kB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai])\n  Obtaining dependency information for requests-toolbelt>=1.0.0 from https://files.pythonhosted.org/packages/3f/51/d4db610ef29373b879047326cbf6fa98b6c1969d6f6dc423279de2b1be2c/requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nCollecting zstandard>=0.23.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai->databricks-sdk[openai])\n  Obtaining dependency information for zstandard>=0.23.0 from https://files.pythonhosted.org/packages/bb/1f/e9cfd801a3f9190bf3e759c422bbfd2247db9d7f3d54a56ecde70137791a/zstandard-0.25.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n  Downloading zstandard-0.25.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: rfc3339-validator in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.1.4)\nRequirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (1.12.0)\nCollecting docutils (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow)\n  Obtaining dependency information for docutils from https://files.pythonhosted.org/packages/66/dd/f95350e853a4468ec37478414fc04ae2d61dad7a947b3015c3dcc51a09b9/docutils-0.22.2-py3-none-any.whl.metadata\n  Downloading docutils-0.22.2-py3-none-any.whl.metadata (15 kB)\nDownloading mlflow-3.4.0-py3-none-any.whl (26.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/26.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/26.7 MB\u001B[0m \u001B[31m110.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.7/26.7 MB\u001B[0m \u001B[31m113.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.5/26.7 MB\u001B[0m \u001B[31m190.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m21.2/26.7 MB\u001B[0m \u001B[31m192.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m172.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m172.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m172.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.7/26.7 MB\u001B[0m \u001B[31m59.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.4.0-py3-none-any.whl (2.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m119.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_tracing-3.4.0-py3-none-any.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m92.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/247.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m247.4/247.4 kB\u001B[0m \u001B[31m21.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading cryptography-45.0.7-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/4.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m226.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m125.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading fastmcp-2.12.3-py3-none-any.whl (314 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/314.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m314.0/314.0 kB\u001B[0m \u001B[31m61.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/114.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m114.9/114.9 kB\u001B[0m \u001B[31m28.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/85.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.0/85.0 kB\u001B[0m \u001B[31m20.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/73.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.5/73.5 kB\u001B[0m \u001B[31m23.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.8/78.8 kB\u001B[0m \u001B[31m23.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/75.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.0/75.0 kB\u001B[0m \u001B[31m17.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading openai-1.108.1-py3-none-any.whl (948 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/948.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m948.4/948.4 kB\u001B[0m \u001B[31m116.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading authlib-1.6.4-py2.py3-none-any.whl (243 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/243.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m243.1/243.1 kB\u001B[0m \u001B[31m44.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading cyclopts-3.24.0-py3-none-any.whl (86 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/86.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.2/86.2 kB\u001B[0m \u001B[31m29.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading jiter-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/349.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m349.0/349.0 kB\u001B[0m \u001B[31m80.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/447.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m447.5/447.5 kB\u001B[0m \u001B[31m75.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mcp-1.14.1-py3-none-any.whl (163 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/163.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m163.8/163.8 kB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading openapi_core-0.19.5-py3-none-any.whl (106 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/106.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.6/106.6 kB\u001B[0m \u001B[31m24.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/96.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.4/96.4 kB\u001B[0m \u001B[31m14.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/65.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.7/65.7 kB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/72.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m72.5/72.5 kB\u001B[0m \u001B[31m21.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/131.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m131.9/131.9 kB\u001B[0m \u001B[31m24.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/208.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m208.0/208.0 kB\u001B[0m \u001B[31m40.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/322.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.0/322.0 kB\u001B[0m \u001B[31m44.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/98.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m98.3/98.3 kB\u001B[0m \u001B[31m27.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pyperclip-1.10.0-py3-none-any.whl (11 kB)\nDownloading rich-14.1.0-py3-none-any.whl (243 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/243.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m243.4/243.4 kB\u001B[0m \u001B[31m50.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m79.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading uvicorn-0.36.0-py3-none-any.whl (67 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/67.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.7/67.7 kB\u001B[0m \u001B[31m18.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/224.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m224.4/224.4 kB\u001B[0m \u001B[31m53.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mako-1.3.10-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m24.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\nDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\nDownloading h11-0.16.0-py3-none-any.whl (37 kB)\nDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading langsmith-0.4.29-py3-none-any.whl (386 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/386.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m386.2/386.2 kB\u001B[0m \u001B[31m77.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/87.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.3/87.3 kB\u001B[0m \u001B[31m25.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/45.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.2/45.2 kB\u001B[0m \u001B[31m14.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading rich_rst-1.3.1-py3-none-any.whl (11 kB)\nDownloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\nDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\nDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/331.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m331.1/331.1 kB\u001B[0m \u001B[31m60.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nDownloading orjson-3.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/132.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.7/132.7 kB\u001B[0m \u001B[31m30.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/54.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.5/54.5 kB\u001B[0m \u001B[31m19.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading zstandard-0.25.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.6 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/5.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m204.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m124.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading docutils-0.22.2-py3-none-any.whl (632 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/632.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m632.7/632.7 kB\u001B[0m \u001B[31m46.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pyperclip, parse, zstandard, werkzeug, python-multipart, pyparsing, protobuf, orjson, mdurl, Mako, jiter, httpx-sse, h11, gunicorn, graphql-relay, exceptiongroup, docutils, docstring-parser, dnspython, uvicorn, tiktoken, sse-starlette, requests-toolbelt, opentelemetry-proto, opentelemetry-api, markdown-it-py, httpcore, graphene, email-validator, cryptography, alembic, rich, pydantic-settings, opentelemetry-semantic-conventions, openapi-pydantic, httpx, authlib, rich-rst, opentelemetry-sdk, openai, mcp, langsmith, mlflow-tracing, mlflow-skinny, langchain-core, cyclopts, openapi-core, langchain-openai, fastmcp, mlflow\n  Attempting uninstall: werkzeug\n    Found existing installation: Werkzeug 3.1.3\n    Not uninstalling werkzeug at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-3ee02f12-33f1-40a2-aefb-144b9e322d9c\n    Can't uninstall 'Werkzeug'. No files were found to uninstall.\n  Attempting uninstall: pyparsing\n    Found existing installation: pyparsing 3.2.5\n    Not uninstalling pyparsing at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-3ee02f12-33f1-40a2-aefb-144b9e322d9c\n    Can't uninstall 'pyparsing'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.24.1\n    Not uninstalling protobuf at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-3ee02f12-33f1-40a2-aefb-144b9e322d9c\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: cryptography\n    Found existing installation: cryptography 41.0.3\n    Not uninstalling cryptography at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-3ee02f12-33f1-40a2-aefb-144b9e322d9c\n    Can't uninstall 'cryptography'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.11.4\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-3ee02f12-33f1-40a2-aefb-144b9e322d9c\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nasn1tools 0.167.0 requires pyparsing>=3.1.2, but you have pyparsing 3.0.9 which is incompatible.\ngoogle-api-core 2.18.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\ngoogleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\nproto-plus 1.24.0 requires protobuf<6.0.0dev,>=3.19.0, but you have protobuf 6.32.1 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed Mako-1.3.10 alembic-1.16.5 authlib-1.6.4 cryptography-45.0.7 cyclopts-3.24.0 dnspython-2.8.0 docstring-parser-0.17.0 docutils-0.22.2 email-validator-2.3.0 exceptiongroup-1.3.0 fastmcp-2.12.3 graphene-3.4.3 graphql-relay-3.2.0 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 jiter-0.11.0 langchain-core-0.3.76 langchain-openai-0.3.33 langsmith-0.4.29 markdown-it-py-4.0.0 mcp-1.14.1 mdurl-0.1.2 mlflow-3.4.0 mlflow-skinny-3.4.0 mlflow-tracing-3.4.0 openai-1.108.1 openapi-core-0.19.5 openapi-pydantic-0.5.1 opentelemetry-api-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 orjson-3.11.3 parse-1.20.2 protobuf-6.32.1 pydantic-settings-2.10.1 pyparsing-3.0.9 pyperclip-1.10.0 python-multipart-0.0.20 requests-toolbelt-1.0.0 rich-14.1.0 rich-rst-1.3.1 sse-starlette-3.0.2 tiktoken-0.11.0 uvicorn-0.36.0 werkzeug-3.1.1 zstandard-0.25.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U mlflow databricks-sdk[openai]\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c07f25-cae9-4ab8-a9ae-9f7eaf3bb6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from typing import Dict, List, Any, Union, Set, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pathlib import Path\n",
    "\n",
    "mlflow.openai.autolog()\n",
    "mlflow.set_experiment(experiment_id=2347514886748068)\n",
    "mlflow.tracing.disable_notebook_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f61c53b-0144-44b9-ab00-bbf24a6981ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### JSON EXTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1629e9cc-d81a-4f18-9cbd-68c5a9d8af28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KeyInfo:\n",
    "    path: str\n",
    "    key: str\n",
    "    type_name: str\n",
    "    value: Any = None\n",
    "\n",
    "@dataclass\n",
    "class JSONKeyExtractor:\n",
    "    keys: Set[str] = field(default_factory=set)\n",
    "    paths: List[str] = field(default_factory=list)\n",
    "    key_types: Dict[str, str] = field(default_factory=dict)\n",
    "    key_value_pairs: Dict[str, Any] = field(default_factory=dict)  # path -> value\n",
    "\n",
    "    def extract_keys(self, data: Union[str, Dict, List], current_path: str = \"\") -> None:\n",
    "        if isinstance(data, str):\n",
    "            try:\n",
    "                data = json.loads(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Invalid JSON: {e}\")\n",
    "        self._extract_recursive(data, current_path)\n",
    "\n",
    "    def _extract_recursive(self, obj: Any, current_path: str = \"\") -> None:\n",
    "        if obj is None:\n",
    "            return\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                new_path = f\"{current_path}.{key}\" if current_path else key\n",
    "                self.keys.add(key)\n",
    "                self.paths.append(new_path)\n",
    "                self.key_types[new_path] = type(value).__name__\n",
    "                self.key_value_pairs[new_path] = value\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    self._extract_recursive(value, new_path)\n",
    "        elif isinstance(obj, list):\n",
    "            for index, item in enumerate(obj):\n",
    "                new_path = f\"{current_path}[{index}]\" if current_path else f\"[{index}]\"\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    self._extract_recursive(item, new_path)\n",
    "\n",
    "    def get_keys(self, \n",
    "                 include_nested: bool = True,\n",
    "                 show_path: bool = False,\n",
    "                 unique_only: bool = True,\n",
    "                 sort_keys: bool = False,\n",
    "                 filter_by_type: Optional[str] = None) -> List[str]:\n",
    "        if show_path:\n",
    "            result = self.paths.copy()\n",
    "            if filter_by_type:\n",
    "                result = [path for path in result \n",
    "                          if self.key_types.get(path, '').lower() == filter_by_type.lower()]\n",
    "        else:\n",
    "            if unique_only:\n",
    "                result = list(self.keys)\n",
    "            else:\n",
    "                result = []\n",
    "                for path in self.paths:\n",
    "                    key = path.split('.')[-1].split('[')[0]\n",
    "                    result.append(key)\n",
    "        if sort_keys:\n",
    "            result.sort()\n",
    "        return result\n",
    "\n",
    "    def get_pairs(self, \n",
    "                  show_path: bool = True,\n",
    "                  filter_by_type: Optional[str] = None,\n",
    "                  sort_keys: bool = False) -> List[Tuple[str, Any]]:\n",
    "        pairs = []\n",
    "        for path in self.paths:\n",
    "            if filter_by_type and self.key_types.get(path, '').lower() != filter_by_type.lower():\n",
    "                continue\n",
    "            key = path if show_path else path.split('.')[-1].split('[')[0]\n",
    "            value = self.key_value_pairs.get(path)\n",
    "            pairs.append((key, value))\n",
    "        if sort_keys:\n",
    "            pairs.sort(key=lambda x: x[0])\n",
    "        return pairs\n",
    "\n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        type_counts = {}\n",
    "        for key_type in self.key_types.values():\n",
    "            type_counts[key_type] = type_counts.get(key_type, 0) + 1\n",
    "        return {\n",
    "            'total_keys': len(self.paths),\n",
    "            'unique_keys': len(self.keys),\n",
    "            'max_depth': max([path.count('.') for path in self.paths] + [0]),\n",
    "            'type_distribution': type_counts\n",
    "        }\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.keys.clear()\n",
    "        self.paths.clear()\n",
    "        self.key_types.clear()\n",
    "        self.key_value_pairs.clear()\n",
    "\n",
    "    def export_results(self, \n",
    "                      format_type: str = 'list',\n",
    "                      include_types: bool = False,\n",
    "                      include_pairs: bool = False,\n",
    "                      **kwargs) -> Union[List[str], Dict[str, Any], str]:\n",
    "        if include_pairs:\n",
    "            pairs = self.get_pairs(\n",
    "                show_path=kwargs.get('show_path', True),\n",
    "                filter_by_type=kwargs.get('filter_by_type', None),\n",
    "                sort_keys=kwargs.get('sort_keys', False)\n",
    "            )\n",
    "            if format_type == 'list':\n",
    "                return [f\"{k}: {v}\" for k, v in pairs]\n",
    "            elif format_type == 'dict':\n",
    "                return {k: v for k, v in pairs}\n",
    "            elif format_type == 'json':\n",
    "                return json.dumps({k: v for k, v in pairs}, indent=2)\n",
    "            elif format_type == 'csv':\n",
    "                lines = ['key,value']\n",
    "                for k, v in pairs:\n",
    "                    lines.append(f'\"{k}\",\"{v}\"')\n",
    "                return '\\n'.join(lines)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {format_type}\")\n",
    "        else:\n",
    "            keys = self.get_keys(**kwargs)\n",
    "            if format_type == 'list':\n",
    "                return keys\n",
    "            elif format_type == 'dict':\n",
    "                if include_types:\n",
    "                    return {path: self.key_types.get(path, 'unknown') \n",
    "                            for path in self.paths}\n",
    "                else:\n",
    "                    return {i: key for i, key in enumerate(keys)}\n",
    "            elif format_type == 'json':\n",
    "                if include_types:\n",
    "                    data = {path: self.key_types.get(path, 'unknown') \n",
    "                            for path in self.paths}\n",
    "                else:\n",
    "                    data = keys\n",
    "                return json.dumps(data, indent=2)\n",
    "            elif format_type == 'csv':\n",
    "                if include_types:\n",
    "                    lines = ['key,type']\n",
    "                    for path in self.paths:\n",
    "                        key_type = self.key_types.get(path, 'unknown')\n",
    "                        lines.append(f'\"{path}\",\"{key_type}\"')\n",
    "                else:\n",
    "                    lines = ['key'] + [f'\"{key}\"' for key in keys]\n",
    "                return '\\n'.join(lines)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {format_type}\")\n",
    "\n",
    "    def get_values_by_key(self, search_key: str) -> List[Any]:\n",
    "        values = []\n",
    "        for path in self.paths:\n",
    "            key = path.split('.')[-1].split('[')[0]\n",
    "            if key == search_key:\n",
    "                values.append(self.key_value_pairs[path])\n",
    "        return values\n",
    "\n",
    "    def key_exists(self, search_key: str) -> bool:\n",
    "        return search_key in self.keys\n",
    "\n",
    "    def get_key_value_pairs(self, search_key: str) -> List[Tuple[str, Any]]:\n",
    "        pairs = []\n",
    "        for path in self.paths:\n",
    "            key = path.split('.')[-1].split('[')[0]\n",
    "            if key == search_key:\n",
    "                pairs.append((path, self.key_value_pairs[path]))\n",
    "        return pairs\n",
    "\n",
    "    def get_subchild_list_items(self, search_key: str) -> Dict[str, List[Any]]:\n",
    "        subchild_items = {}\n",
    "        for path in self.paths:\n",
    "            key = path.split('.')[-1].split('[')[0]\n",
    "            if key == search_key and isinstance(self.key_value_pairs[path], list):\n",
    "                subchild_items[path] = self.key_value_pairs[path]\n",
    "        return subchild_items\n",
    "\n",
    "def print_statistics(stats: Dict[str, Any]):\n",
    "    print(\"\\n=== Statistics ===\")\n",
    "    print(f\"Total keys: {stats['total_keys']}\")\n",
    "    print(f\"Unique keys: {stats['unique_keys']}\")\n",
    "    print(f\"Max depth: {stats['max_depth']}\")\n",
    "    print(\"\\nType distribution:\")\n",
    "    for type_name, count in sorted(stats['type_distribution'].items()):\n",
    "        print(f\"  {type_name}: {count}\")\n",
    "    print()\n",
    "\n",
    "def main_extractor(input_path, key, feature):\n",
    "    input_path = input_path\n",
    "    search_key = key\n",
    "    feature = feature\n",
    "    output_path = \"output_keys.txt\"\n",
    "    output_format = \"list\"\n",
    "    show_path = False\n",
    "    include_types = False\n",
    "    sort_keys = False\n",
    "    unique_only = True\n",
    "    filter_by_type = None\n",
    "    show_stats = True\n",
    "    include_pairs = True \n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Error: File not found: {input_path}\")\n",
    "        return\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return\n",
    "\n",
    "    extractor = JSONKeyExtractor()\n",
    "    try:\n",
    "        extractor.extract_keys(json_data)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    if not extractor.paths:\n",
    "        print(\"No keys found in the JSON data\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        results = extractor.export_results(\n",
    "            format_type=output_format,\n",
    "            show_path=show_path,\n",
    "            include_types=include_types,\n",
    "            sort_keys=sort_keys,\n",
    "            unique_only=unique_only,\n",
    "            filter_by_type=filter_by_type,\n",
    "            include_pairs=include_pairs\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # output_text = \"\"\n",
    "    # if isinstance(results, list):\n",
    "    #     output_text = '\\n'.join(results)\n",
    "    # else:\n",
    "    #     output_text = str(results)\n",
    "\n",
    "    # try:\n",
    "    #     with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    #         f.write(output_text)\n",
    "    #     print(f\"Results saved to {output_path}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error writing to file: {e}\")\n",
    "    #     return\n",
    "\n",
    "    # if show_stats:\n",
    "    #     stats = extractor.get_statistics()\n",
    "    #     print_statistics(stats)\n",
    "\n",
    "    if feature == \"1\":\n",
    "        exists = extractor.key_exists(search_key)\n",
    "        if exists:\n",
    "            # print(f\"✅ Key '{search_key}' exists in the JSON.\")\n",
    "            return search_key\n",
    "        else:\n",
    "            print(f\"❌ Key '{search_key}' does NOT exist in the JSON.\")\n",
    "    elif feature == \"2\":\n",
    "        values = extractor.get_values_by_key(search_key)\n",
    "        if values:\n",
    "            for idx, value in enumerate(values):\n",
    "                # print(f\"  [{idx}]: {value}\")\n",
    "                return value\n",
    "        else:\n",
    "            print(f\"No values found for key '{search_key}'.\")\n",
    "    elif feature == \"3\":\n",
    "        subchilds = extractor.get_subchild_list_items(search_key)\n",
    "        if not subchilds:\n",
    "            # Try to find subchilds by matching full path if search_key is a path\n",
    "            for path in extractor.paths:\n",
    "                if search_key in path and isinstance(extractor.key_value_pairs[path], list):\n",
    "                    subchilds[path] = extractor.key_value_pairs[path]\n",
    "        for path, items in subchilds.items():\n",
    "            for idx, item in enumerate(items):\n",
    "                # print(f\"    [{idx}]: {item}\")\n",
    "                return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76081abd-33c6-46a8-9ef3-dd829d3b528c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CONFIG VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "538a45cf-58a6-4226-bb97-7aca034318a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_schema_from_check_schema_consistency(data_quality_configuration):\n",
    "    schema_dict = {}\n",
    "    for rule in data_quality_configuration:\n",
    "        if rule.get(\"rule_name\") == \"check_schema_consistency\":\n",
    "            for col in rule.get(\"parameters\", {}).get(\"expected_schema\", []):\n",
    "                column_name = col.get(\"column_name\")\n",
    "                data_type = col.get(\"data_type\")\n",
    "                if column_name and data_type:\n",
    "                    schema_dict[column_name] = data_type\n",
    "    return schema_dict\n",
    "\n",
    "def test_schema_matches(schema_dict, schema_kv):\n",
    "    results = []\n",
    "    for col, expected_type in schema_dict.items():\n",
    "        actual_type = schema_kv.get(col)\n",
    "        if actual_type is None:\n",
    "            if col == \"file_id\":\n",
    "                continue\n",
    "            results.append({\n",
    "                \"status\": \"❌\",\n",
    "                \"config_column\": col,\n",
    "                \"config_datatype\": expected_type,\n",
    "                \"table_column\": None,\n",
    "                \"table_datatype\": None\n",
    "            })\n",
    "        elif actual_type.lower() == expected_type.lower():\n",
    "            results.append({\n",
    "                \"status\": \"✅\",\n",
    "                \"config_column\": col,\n",
    "                \"config_datatype\": expected_type,\n",
    "                \"table_column\": col,\n",
    "                \"table_datatype\": actual_type\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"status\": \"❌\",\n",
    "                \"config_column\": col,\n",
    "                \"config_datatype\": expected_type,\n",
    "                \"table_column\": col,\n",
    "                \"table_datatype\": actual_type\n",
    "            })\n",
    "    for col in schema_kv:\n",
    "        if col not in schema_dict:\n",
    "            results.append({\n",
    "                \"status\": \"⚠️\",\n",
    "                \"config_column\": None,\n",
    "                \"config_datatype\": None,\n",
    "                \"table_column\": col,\n",
    "                \"table_datatype\": schema_kv[col]\n",
    "            })\n",
    "    display(pd.DataFrame(results))\n",
    "    # df = pd.DataFrame(results)\n",
    "    # display(df)\n",
    "    # for _, row in df.iterrows():\n",
    "    #     # log_and_report(job, \"Schema consistency\", f\"{row.to_dict()}\")\n",
    "    #     log_and_report(job, \"Schema consistency\", f\"{row}\")\n",
    "    all(r[\"status\"] == \"✅\" or r[\"status\"] == \"⚠️\" for r in results), \"Schema mismatch found between expected and actual table schema\"\n",
    "\n",
    "    print()\n",
    "    # print(f\"{'-'*55} CHECKING OF FIELD TOTAL COUNT FROM CONFIG VS CATALOG TABLE {'-'*55}\")\n",
    "    schema_dict_cnt = sum(1 for col in schema_dict)\n",
    "    schema_kv_cnt = sum(1 for col in schema_kv)\n",
    "    if schema_dict_cnt != schema_kv_cnt +1:\n",
    "        print(f\"❌ Column count mismatch: config={schema_dict_cnt}, table={schema_kv_cnt}\")\n",
    "        print(f\"❌ It should not be equal, the total count from config should be +1 vs the total count from table because of file_id\")\n",
    "    else:\n",
    "        print(f\"✅ Config Json schema column count is : {schema_dict_cnt} because of file_id\")\n",
    "        print(f\"✅ Catalog Table schema column count: {schema_kv_cnt} because we dont include file_id in table\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "def extract_partition_columns_from_create_table(dev_tbl_df):\n",
    "    create_table_stmt = spark.sql(f\"show create table {dev_tbl_df}\").collect()[0][0]\n",
    "    partitioned_by_match = re.search(r\"PARTITIONED BY\\s*\\((.*?)\\)\", create_table_stmt, re.DOTALL)\n",
    "    if partitioned_by_match:\n",
    "        partition_cols = [col.strip().split()[0] for col in partitioned_by_match.group(1).split(\",\")]\n",
    "        # print(partition_cols)\n",
    "        return partition_cols\n",
    "    else:\n",
    "        print(\"No PARTITIONED BY clause found.\")\n",
    "        return []\n",
    "\n",
    "# display(spark.sql(\"show create table wde_dev.cu_b.opt_dly_descriptions\"))\n",
    "\n",
    "\n",
    "def rules_for_parquet(config, file_format):\n",
    "    allowed_rules = {\n",
    "        \"compare_file_size_in_bytes\",\n",
    "        \"reconcile_file_set\",\n",
    "        \"check_schema_consistency\",\n",
    "        \"check_column_count\",\n",
    "        \"check_null\",\n",
    "    }\n",
    "    rule_names = [rule.get(\"rule_name\") for rule in config]\n",
    "    if file_format.lower() == \"parquet\":\n",
    "        invalid_rules = [r for r in rule_names if r not in allowed_rules]\n",
    "        if invalid_rules:\n",
    "            print(f\"❌ Invalid rules for parquet: {invalid_rules}\")\n",
    "        else:\n",
    "            print(f\"✅ All rules are valid for {file_format}: {rule_names}\")\n",
    "    else:\n",
    "        print(rule_names)\n",
    "        print(file_format)\n",
    "\n",
    "def rules_for_delimited_file(config, file_format, delimiter):\n",
    "\n",
    "    if file_format.lower() in [\"csv\", \"tsv\", \"txt\"]:\n",
    "        if not delimiter:\n",
    "            print(f\"❌ file_format '{file_format}' should have a non-empty delimiter.\")\n",
    "        else:\n",
    "            print(f\"✅ Delimiter for file_format '{file_format}': '{delimiter}'\")\n",
    "    allowed_rules = {\n",
    "        \"compare_file_size_in_bytes\",\n",
    "        \"reconcile_file_set\",\n",
    "        \"check_schema_consistency\",\n",
    "        \"check_column_count\",\n",
    "        \"check_null\",\n",
    "        \"compare_row_count\"\n",
    "    }\n",
    "    rule_names = [rule.get(\"rule_name\") for rule in config]\n",
    "    if file_format.lower() in [\"csv\", \"tsv\", \"txt\"]:\n",
    "        invalid_rules = [r for r in rule_names if r not in allowed_rules]\n",
    "        if invalid_rules:\n",
    "            print(f\"❌ Invalid rules for parquet: {invalid_rules}\")\n",
    "        else:\n",
    "            print(f\"✅ All rules are valid for {file_format} : {rule_names}\")\n",
    "    else:\n",
    "        print(rule_names)\n",
    "        print(file_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d05a70-314a-4af6-9230-8ea912ed3473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### AUTO GENERATED FOR COMMENTS USING AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ee4905-f2fe-49a8-8de3-e1429c6eba6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    return Path(file_path).read_text()\n",
    "\n",
    "def generate_output(client, model, user_content, choice_idx=0):\n",
    "    system_content_template = (\n",
    "        \"You are an AI automation assistant specialized in generating helpful, human-readable \"\n",
    "        \"descriptions and comments for database columns or schema fields you can base under catalog. \"\n",
    "        \"Your primary goal is to provide concise, clear, and contextually accurate suggestions short description\"\n",
    "        \"that explain the purpose or meaning of a column, making it easier for users and developers \"\n",
    "        \"to understand the data. \"\n",
    "        \"Descriptions should be professional, unambiguous, and aligned with best practices for \"\n",
    "        \"data documentation. Avoid redundancy and focus on clarity. \"\n",
    "        \"If the column name is technical, expand it into plain language; \"\n",
    "        \"if its ambiguous, infer meaning from context. \"\n",
    "        \"Always use consistent tone and style across descriptions. \"\n",
    "        \"The goal is to minimize human guesswork and improve overall data understanding.\"\n",
    "        \"last reminder dont include double quotes and single quotes do not enclosed the result to double quotes nor single qoutes\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content_template},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[choice_idx].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce61733-f842-4924-9808-541736e70e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Description / Comment and tags validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6d5f94-e2dd-40f7-a670-90289959b7f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_column_comment(table_name: str, column_name: str, comment: str):\n",
    "    sql = f\"ALTER TABLE {table_name} ALTER COLUMN {column_name} COMMENT '{comment}'\"\n",
    "    spark.sql(sql)\n",
    "    print(f\"ALTER TABLE {table_name} ALTER COLUMN {column_name} COMMENT '{comment}'\")\n",
    "\n",
    "def add_column_tag(table_name: str, column_name: str, tag_key: str, tag_value: str):\n",
    "#    sql = f\"ALTER TABLE {table_name} ALTER COLUMN {column_name} SET TAGS ('{tag_key}' = '{tag_value}')\"\n",
    "#    spark.sql(sql)\n",
    "    print(f\"ALTER TABLE {table_name} ALTER COLUMN {column_name} SET TAGS ('{tag_key}' = '{tag_value}')\")\n",
    "   \n",
    "\n",
    "def check_table_column_comments(table_name=\"\", tags_file_path=\"GDM_Databricks_Tag_List.csv\", model_to_use=\"\"):\n",
    "\n",
    "    tags_file_path = tags_file_path\n",
    "\n",
    "    df_spark = spark.sql(f\"DESCRIBE EXTENDED {table_name}\")\n",
    "    properties_not_to_be_include = ['Catalog','Database','Table','Created Time','Last Access','Created By','Type','Location','Provider','Owner','Is_managed_location','Predictive Optimization','Table Properties','# col_name', 'Column Names', 'Column Selection Method', 'Statistics']\n",
    "    columns_section = df_spark.where(\"col_name != '' and data_type != ''\")\n",
    "    columns = columns_section.select(\"col_name\", \"comment\").collect()\n",
    "    results = []\n",
    "\n",
    "    df_csv = pd.read_csv(tags_file_path)\n",
    "\n",
    "    for _, tag_row in df_csv.iterrows():\n",
    "        tag_content = tag_row.to_dict()\n",
    "        field_names = str(tag_content.get('field_name'))\n",
    "        tag_value = str(tag_content.get('tag_value'))\n",
    "        tag_key = str(tag_content.get('tag_key'))\n",
    "        tag_status = \"\"\n",
    "        tag_command = \"\"\n",
    "        for row in columns:\n",
    "            col_name = row[\"col_name\"]\n",
    "            comment = row[\"comment\"]\n",
    "\n",
    "            if col_name in properties_not_to_be_include:\n",
    "                continue\n",
    "            if any(r[\"column\"] == col_name for r in results):\n",
    "                continue\n",
    "\n",
    "            status = \"\"\n",
    "            if comment and str(comment).strip() :\n",
    "                status = \"✅\"\n",
    "\n",
    "                if col_name == field_names:\n",
    "                    tag_status = \"✅\"\n",
    "                    tag_command = f\"ALTER TABLE {table_name} ALTER COLUMN {col_name} SET TAGS ('{tag_key}' = '{tag_value}');\"\n",
    "                    # add_column_tag(table_name=table_name, column_name=col_name, tag_key=tag_key, tag_value=tag_value)\n",
    "                elif col_name != field_names:\n",
    "                    tag_status = \"\"\n",
    "                    tag_command = \"\"\n",
    "                else:    \n",
    "                    tag_status = \"\"\n",
    "\n",
    "            else:\n",
    "                status = \"❌\"\n",
    "\n",
    "\n",
    "            if  status == \"❌\":\n",
    "                user_content = f\"\"\" give a suggestion for this column/field {col_name} make it and prefessional that follows data governance you can base on the catalog, lastly the result should be one line sentence give me only the suggested description because I dont have to see the other text\"\"\"\n",
    "                suggested_comment = generate_output(\n",
    "                    client,\n",
    "                    model_to_use,\n",
    "                    user_content,\n",
    "                    choice_idx=0\n",
    "                )\n",
    "                # add_column_comment(table_name=table_name, column_name=col_name, comment=suggested_comment)\n",
    "            else:\n",
    "                suggested_comment = \"\"\n",
    "\n",
    "            results.append({\n",
    "                \"column\": col_name,\n",
    "                \"has_comment\": status,\n",
    "                \"comment\": comment,\n",
    "                \"suggested_comment\": suggested_comment,\n",
    "                \"tags_status\": tag_status,\n",
    "                \"tags_command\": tag_command\n",
    "            })\n",
    "\n",
    "    display(pd.DataFrame(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ea19e8-c8e8-48fb-9f8a-b0f0d4c7bcaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de4d49d-42de-490b-be3a-954300664b55",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"suggested_comment\":154,\"comment\":369,\"tags_command\":698,\"has_comment\":122},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758508505465}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n                               cox_nrt_adjustment                               \n################################################################################\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>status</th><th>config_column</th><th>config_datatype</th><th>table_column</th><th>table_datatype</th></tr></thead><tbody><tr><td>✅</td><td>referenceid</td><td>string</td><td>referenceid</td><td>string</td></tr><tr><td>✅</td><td>seqno</td><td>bigint</td><td>seqno</td><td>bigint</td></tr><tr><td>✅</td><td>adjustmenttypename</td><td>string</td><td>adjustmenttypename</td><td>string</td></tr><tr><td>✅</td><td>adjustmentreasonname</td><td>string</td><td>adjustmentreasonname</td><td>string</td></tr><tr><td>✅</td><td>adjustmentreasonproductname</td><td>string</td><td>adjustmentreasonproductname</td><td>string</td></tr><tr><td>✅</td><td>source</td><td>string</td><td>source</td><td>string</td></tr><tr><td>✅</td><td>adjustmenttime</td><td>timestamp</td><td>adjustmenttime</td><td>timestamp</td></tr><tr><td>✅</td><td>localadjustmenttime</td><td>timestamp</td><td>localadjustmenttime</td><td>timestamp</td></tr><tr><td>✅</td><td>accountnum</td><td>bigint</td><td>accountnum</td><td>bigint</td></tr><tr><td>✅</td><td>subscriptionnum</td><td>bigint</td><td>subscriptionnum</td><td>bigint</td></tr><tr><td>✅</td><td>msisdn</td><td>string</td><td>msisdn</td><td>string</td></tr><tr><td>✅</td><td>imsi</td><td>string</td><td>imsi</td><td>string</td></tr><tr><td>✅</td><td>subscriptionid</td><td>string</td><td>subscriptionid</td><td>string</td></tr><tr><td>✅</td><td>callingcardserialno</td><td>bigint</td><td>callingcardserialno</td><td>bigint</td></tr><tr><td>✅</td><td>callingcardnumberseries</td><td>string</td><td>callingcardnumberseries</td><td>string</td></tr><tr><td>✅</td><td>otheraccountnum</td><td>bigint</td><td>otheraccountnum</td><td>bigint</td></tr><tr><td>✅</td><td>amount</td><td>float</td><td>amount</td><td>float</td></tr><tr><td>✅</td><td>taxamount</td><td>float</td><td>taxamount</td><td>float</td></tr><tr><td>✅</td><td>countertaxamount</td><td>float</td><td>countertaxamount</td><td>float</td></tr><tr><td>✅</td><td>taxuuid</td><td>string</td><td>taxuuid</td><td>string</td></tr><tr><td>✅</td><td>taxname</td><td>string</td><td>taxname</td><td>string</td></tr><tr><td>✅</td><td>currency</td><td>string</td><td>currency</td><td>string</td></tr><tr><td>✅</td><td>adminname</td><td>string</td><td>adminname</td><td>string</td></tr><tr><td>✅</td><td>vouchernum</td><td>bigint</td><td>vouchernum</td><td>bigint</td></tr><tr><td>✅</td><td>vouchertypeid</td><td>string</td><td>vouchertypeid</td><td>string</td></tr><tr><td>✅</td><td>voucherpin</td><td>string</td><td>voucherpin</td><td>string</td></tr><tr><td>✅</td><td>additionalinfo</td><td>string</td><td>additionalinfo</td><td>string</td></tr><tr><td>✅</td><td>accounttypeid</td><td>string</td><td>accounttypeid</td><td>string</td></tr><tr><td>✅</td><td>accounttypename</td><td>string</td><td>accounttypename</td><td>string</td></tr><tr><td>✅</td><td>subscriptiontypename</td><td>string</td><td>subscriptiontypename</td><td>string</td></tr><tr><td>✅</td><td>accessmethodnamekey</td><td>string</td><td>accessmethodnamekey</td><td>string</td></tr><tr><td>✅</td><td>status</td><td>int</td><td>status</td><td>int</td></tr><tr><td>✅</td><td>externaltransactionreference</td><td>string</td><td>externaltransactionreference</td><td>string</td></tr><tr><td>✅</td><td>postbalance</td><td>float</td><td>postbalance</td><td>float</td></tr><tr><td>✅</td><td>subscriptionproperty</td><td>string</td><td>subscriptionproperty</td><td>string</td></tr><tr><td>✅</td><td>recordtags</td><td>string</td><td>recordtags</td><td>string</td></tr><tr><td>❌</td><td>txn_dt</td><td>date</td><td>null</td><td>null</td></tr><tr><td>❌</td><td>file_date</td><td>date</td><td>null</td><td>null</td></tr><tr><td>✅</td><td>dbx_process_dttm</td><td>timestamp</td><td>dbx_process_dttm</td><td>timestamp</td></tr><tr><td>✅</td><td>file_name</td><td>string</td><td>file_name</td><td>string</td></tr><tr><td>⚠️</td><td>null</td><td>null</td><td>txn_date</td><td>date</td></tr><tr><td>⚠️</td><td>null</td><td>null</td><td>file_timestamp_bucket</td><td>timestamp</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "✅",
         "referenceid",
         "string",
         "referenceid",
         "string"
        ],
        [
         "✅",
         "seqno",
         "bigint",
         "seqno",
         "bigint"
        ],
        [
         "✅",
         "adjustmenttypename",
         "string",
         "adjustmenttypename",
         "string"
        ],
        [
         "✅",
         "adjustmentreasonname",
         "string",
         "adjustmentreasonname",
         "string"
        ],
        [
         "✅",
         "adjustmentreasonproductname",
         "string",
         "adjustmentreasonproductname",
         "string"
        ],
        [
         "✅",
         "source",
         "string",
         "source",
         "string"
        ],
        [
         "✅",
         "adjustmenttime",
         "timestamp",
         "adjustmenttime",
         "timestamp"
        ],
        [
         "✅",
         "localadjustmenttime",
         "timestamp",
         "localadjustmenttime",
         "timestamp"
        ],
        [
         "✅",
         "accountnum",
         "bigint",
         "accountnum",
         "bigint"
        ],
        [
         "✅",
         "subscriptionnum",
         "bigint",
         "subscriptionnum",
         "bigint"
        ],
        [
         "✅",
         "msisdn",
         "string",
         "msisdn",
         "string"
        ],
        [
         "✅",
         "imsi",
         "string",
         "imsi",
         "string"
        ],
        [
         "✅",
         "subscriptionid",
         "string",
         "subscriptionid",
         "string"
        ],
        [
         "✅",
         "callingcardserialno",
         "bigint",
         "callingcardserialno",
         "bigint"
        ],
        [
         "✅",
         "callingcardnumberseries",
         "string",
         "callingcardnumberseries",
         "string"
        ],
        [
         "✅",
         "otheraccountnum",
         "bigint",
         "otheraccountnum",
         "bigint"
        ],
        [
         "✅",
         "amount",
         "float",
         "amount",
         "float"
        ],
        [
         "✅",
         "taxamount",
         "float",
         "taxamount",
         "float"
        ],
        [
         "✅",
         "countertaxamount",
         "float",
         "countertaxamount",
         "float"
        ],
        [
         "✅",
         "taxuuid",
         "string",
         "taxuuid",
         "string"
        ],
        [
         "✅",
         "taxname",
         "string",
         "taxname",
         "string"
        ],
        [
         "✅",
         "currency",
         "string",
         "currency",
         "string"
        ],
        [
         "✅",
         "adminname",
         "string",
         "adminname",
         "string"
        ],
        [
         "✅",
         "vouchernum",
         "bigint",
         "vouchernum",
         "bigint"
        ],
        [
         "✅",
         "vouchertypeid",
         "string",
         "vouchertypeid",
         "string"
        ],
        [
         "✅",
         "voucherpin",
         "string",
         "voucherpin",
         "string"
        ],
        [
         "✅",
         "additionalinfo",
         "string",
         "additionalinfo",
         "string"
        ],
        [
         "✅",
         "accounttypeid",
         "string",
         "accounttypeid",
         "string"
        ],
        [
         "✅",
         "accounttypename",
         "string",
         "accounttypename",
         "string"
        ],
        [
         "✅",
         "subscriptiontypename",
         "string",
         "subscriptiontypename",
         "string"
        ],
        [
         "✅",
         "accessmethodnamekey",
         "string",
         "accessmethodnamekey",
         "string"
        ],
        [
         "✅",
         "status",
         "int",
         "status",
         "int"
        ],
        [
         "✅",
         "externaltransactionreference",
         "string",
         "externaltransactionreference",
         "string"
        ],
        [
         "✅",
         "postbalance",
         "float",
         "postbalance",
         "float"
        ],
        [
         "✅",
         "subscriptionproperty",
         "string",
         "subscriptionproperty",
         "string"
        ],
        [
         "✅",
         "recordtags",
         "string",
         "recordtags",
         "string"
        ],
        [
         "❌",
         "txn_dt",
         "date",
         null,
         null
        ],
        [
         "❌",
         "file_date",
         "date",
         null,
         null
        ],
        [
         "✅",
         "dbx_process_dttm",
         "timestamp",
         "dbx_process_dttm",
         "timestamp"
        ],
        [
         "✅",
         "file_name",
         "string",
         "file_name",
         "string"
        ],
        [
         "⚠️",
         null,
         null,
         "txn_date",
         "date"
        ],
        [
         "⚠️",
         null,
         null,
         "file_timestamp_bucket",
         "timestamp"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "config_column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "config_datatype",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_datatype",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ Config Json schema column count is : 41 because of file_id\n✅ Catalog Table schema column count: 40 because we dont include file_id in table\n\n\n❌ Partition or Column 'txn_dt' NOT found in table wde_dev.cu_b.cox_nrt_adjustment\n❌ Partition or Column 'file_date' NOT found in table wde_dev.cu_b.cox_nrt_adjustment\n✅ Delimiter for file_format 'csv': ';'\n✅ All rules are valid for csv : ['compare_file_size_in_bytes', 'reconcile_file_set', 'check_schema_consistency', 'check_column_count']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>column</th><th>has_comment</th><th>comment</th><th>suggested_comment</th><th>tags_status</th><th>tags_command</th></tr></thead><tbody><tr><td>referenceid</td><td>✅</td><td>A unique identifier for each adjustment made to a customer account</td><td></td><td></td><td></td></tr><tr><td>seqno</td><td>✅</td><td>A unique sequential number assigned to each adjustment made to a customer account</td><td></td><td></td><td></td></tr><tr><td>adjustmenttypename</td><td>✅</td><td>A categorization of the type of adjustment made to a customer account.</td><td></td><td></td><td></td></tr><tr><td>adjustmentreasonname</td><td>✅</td><td>A brief description of the reason behind each adjustment made to a customer account</td><td></td><td></td><td></td></tr><tr><td>adjustmentreasonproductname</td><td>✅</td><td>A designation or name of the product associated with the specific reason for the adjustment made to a customer account.</td><td></td><td></td><td></td></tr><tr><td>source</td><td>✅</td><td>Indicates the origin of the customer account adjustment, offering insights into the initiation channel or system involved.</td><td></td><td></td><td></td></tr><tr><td>adjustmenttime</td><td>✅</td><td>Timestamp indicating the date and time when the adjustment was made to the customer account</td><td></td><td></td><td></td></tr><tr><td>localadjustmenttime</td><td>✅</td><td>Local timestamp of adjustment</td><td></td><td></td><td></td></tr><tr><td>accountnum</td><td>✅</td><td>Unique identifier for customer account tied to adjustment, aids in tracking and analyzing account-specific adjustments.</td><td></td><td></td><td></td></tr><tr><td>subscriptionnum</td><td>✅</td><td>A unique identifier for the subscription associated with the customer account adjustment. Helps in linking adjustments to specific subscription services.</td><td></td><td></td><td></td></tr><tr><td>msisdn</td><td>✅</td><td>a unique identifier for a mobile phone number in a telecommunications network.</td><td></td><td>✅</td><td>ALTER TABLE wde_dev.cu_b.cox_nrt_adjustment ALTER COLUMN msisdn SET TAGS ('dataprivacy' = 'personal');</td></tr><tr><td>imsi</td><td>✅</td><td>Unique identifier for a SIM card within a mobile network.</td><td></td><td></td><td></td></tr><tr><td>subscriptionid</td><td>✅</td><td>Unique identifier for the related subscription service.</td><td></td><td></td><td></td></tr><tr><td>callingcardserialno</td><td>✅</td><td>Unique serial number for calling cards used in adjustments.</td><td></td><td></td><td></td></tr><tr><td>callingcardnumberseries</td><td>✅</td><td>A string representing the series or range of numbers associated with calling cards used in adjustments.</td><td></td><td></td><td></td></tr><tr><td>otheraccountnum</td><td>✅</td><td>Secondary unique identifier for another customer account linked to the adjustment.</td><td></td><td></td><td></td></tr><tr><td>amount</td><td>✅</td><td>Amount of the adjustment made to the customer account, representing the financial impact.</td><td></td><td></td><td></td></tr><tr><td>taxamount</td><td>✅</td><td>The taxamount column represents the tax figure associated with the adjustment made to a customer account, reflecting the tax cost or benefit incurred due to the adjustment.</td><td></td><td></td><td></td></tr><tr><td>countertaxamount</td><td>✅</td><td>Counterpart tax figure linked to customer account adjustment, reflecting tax cost or benefit.</td><td></td><td></td><td></td></tr><tr><td>taxuuid</td><td>✅</td><td>A unique identifier for the tax associated with the adjustment made to a customer account. Helps in tracking and associating specific tax details with the adjustment transactions.</td><td></td><td></td><td></td></tr><tr><td>taxname</td><td>✅</td><td>Identifies the tax type linked to the adjustment on a customer account.</td><td></td><td></td><td></td></tr><tr><td>currency</td><td>✅</td><td>Currency used for adjustment amount and tax figures, essential for financial impact analysis.</td><td></td><td></td><td></td></tr><tr><td>adminname</td><td>✅</td><td>Identifier of the administrator initiating the customer account adjustment.</td><td></td><td></td><td></td></tr><tr><td>vouchernum</td><td>✅</td><td>A unique identifier for vouchers used in adjustments made to customer accounts, aiding in tracking and analyzing voucher-specific adjustments.</td><td></td><td></td><td></td></tr><tr><td>vouchertypeid</td><td>✅</td><td>Identifier for the voucher type linked to the customer account adjustment.</td><td></td><td></td><td></td></tr><tr><td>voucherpin</td><td>✅</td><td>Identifier for a voucher utilized in customer account adjustments, detailing voucher usage and its effects on the account.</td><td></td><td></td><td></td></tr><tr><td>additionalinfo</td><td>✅</td><td>Supplementary notes or extra details related to the customer account adjustment.</td><td></td><td></td><td></td></tr><tr><td>accounttypeid</td><td>✅</td><td>Accounttypeid specifies the category of the customer account for classification purposes.</td><td></td><td></td><td></td></tr><tr><td>accounttypename</td><td>✅</td><td>Categorization of the account type impacted by the adjustment.</td><td></td><td></td><td></td></tr><tr><td>subscriptiontypename</td><td>✅</td><td>Identifies the type of subscription linked to the customer account adjustment.</td><td></td><td></td><td></td></tr><tr><td>accessmethodnamekey</td><td>✅</td><td>Key identifier for access method used in adjustments to customer account.</td><td></td><td></td><td></td></tr><tr><td>status</td><td>✅</td><td>Indicates the current status of the customer account adjustment, reflecting if its processed, pending, approved, or declined.</td><td></td><td></td><td></td></tr><tr><td>externaltransactionreference</td><td>✅</td><td>Identifier for an external transaction linked to the customer account adjustment, aiding in external tracking and system integration.</td><td></td><td></td><td></td></tr><tr><td>postbalance</td><td>✅</td><td>Final account balance after adjustment, showing updated financial position.</td><td></td><td></td><td></td></tr><tr><td>subscriptionproperty</td><td>✅</td><td>Specification of subscription property for adjustment.</td><td></td><td></td><td></td></tr><tr><td>recordtags</td><td>✅</td><td>Additional tags for categorization and sorting of adjustment records</td><td></td><td></td><td></td></tr><tr><td>txn_date</td><td>✅</td><td>txn_date partition</td><td></td><td></td><td></td></tr><tr><td>file_timestamp_bucket</td><td>✅</td><td>Represents the timestamp bucket derived from the current processing time, useful for grouping or partitioning data based on processing intervals</td><td></td><td></td><td></td></tr><tr><td>dbx_process_dttm</td><td>✅</td><td>Records the date and time when the data was processed in the DBX system, important for tracking data processing activities.</td><td></td><td></td><td></td></tr><tr><td>file_name</td><td>✅</td><td>Contains the name of the file from which the data was derived, aiding in data traceability and management</td><td></td><td></td><td></td></tr><tr><td>Comment</td><td>❌</td><td></td><td>Free-text comments or remarks provided by users or stakeholders regarding a specific record or transaction.</td><td></td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "referenceid",
         "✅",
         "A unique identifier for each adjustment made to a customer account",
         "",
         "",
         ""
        ],
        [
         "seqno",
         "✅",
         "A unique sequential number assigned to each adjustment made to a customer account",
         "",
         "",
         ""
        ],
        [
         "adjustmenttypename",
         "✅",
         "A categorization of the type of adjustment made to a customer account.",
         "",
         "",
         ""
        ],
        [
         "adjustmentreasonname",
         "✅",
         "A brief description of the reason behind each adjustment made to a customer account",
         "",
         "",
         ""
        ],
        [
         "adjustmentreasonproductname",
         "✅",
         "A designation or name of the product associated with the specific reason for the adjustment made to a customer account.",
         "",
         "",
         ""
        ],
        [
         "source",
         "✅",
         "Indicates the origin of the customer account adjustment, offering insights into the initiation channel or system involved.",
         "",
         "",
         ""
        ],
        [
         "adjustmenttime",
         "✅",
         "Timestamp indicating the date and time when the adjustment was made to the customer account",
         "",
         "",
         ""
        ],
        [
         "localadjustmenttime",
         "✅",
         "Local timestamp of adjustment",
         "",
         "",
         ""
        ],
        [
         "accountnum",
         "✅",
         "Unique identifier for customer account tied to adjustment, aids in tracking and analyzing account-specific adjustments.",
         "",
         "",
         ""
        ],
        [
         "subscriptionnum",
         "✅",
         "A unique identifier for the subscription associated with the customer account adjustment. Helps in linking adjustments to specific subscription services.",
         "",
         "",
         ""
        ],
        [
         "msisdn",
         "✅",
         "a unique identifier for a mobile phone number in a telecommunications network.",
         "",
         "✅",
         "ALTER TABLE wde_dev.cu_b.cox_nrt_adjustment ALTER COLUMN msisdn SET TAGS ('dataprivacy' = 'personal');"
        ],
        [
         "imsi",
         "✅",
         "Unique identifier for a SIM card within a mobile network.",
         "",
         "",
         ""
        ],
        [
         "subscriptionid",
         "✅",
         "Unique identifier for the related subscription service.",
         "",
         "",
         ""
        ],
        [
         "callingcardserialno",
         "✅",
         "Unique serial number for calling cards used in adjustments.",
         "",
         "",
         ""
        ],
        [
         "callingcardnumberseries",
         "✅",
         "A string representing the series or range of numbers associated with calling cards used in adjustments.",
         "",
         "",
         ""
        ],
        [
         "otheraccountnum",
         "✅",
         "Secondary unique identifier for another customer account linked to the adjustment.",
         "",
         "",
         ""
        ],
        [
         "amount",
         "✅",
         "Amount of the adjustment made to the customer account, representing the financial impact.",
         "",
         "",
         ""
        ],
        [
         "taxamount",
         "✅",
         "The taxamount column represents the tax figure associated with the adjustment made to a customer account, reflecting the tax cost or benefit incurred due to the adjustment.",
         "",
         "",
         ""
        ],
        [
         "countertaxamount",
         "✅",
         "Counterpart tax figure linked to customer account adjustment, reflecting tax cost or benefit.",
         "",
         "",
         ""
        ],
        [
         "taxuuid",
         "✅",
         "A unique identifier for the tax associated with the adjustment made to a customer account. Helps in tracking and associating specific tax details with the adjustment transactions.",
         "",
         "",
         ""
        ],
        [
         "taxname",
         "✅",
         "Identifies the tax type linked to the adjustment on a customer account.",
         "",
         "",
         ""
        ],
        [
         "currency",
         "✅",
         "Currency used for adjustment amount and tax figures, essential for financial impact analysis.",
         "",
         "",
         ""
        ],
        [
         "adminname",
         "✅",
         "Identifier of the administrator initiating the customer account adjustment.",
         "",
         "",
         ""
        ],
        [
         "vouchernum",
         "✅",
         "A unique identifier for vouchers used in adjustments made to customer accounts, aiding in tracking and analyzing voucher-specific adjustments.",
         "",
         "",
         ""
        ],
        [
         "vouchertypeid",
         "✅",
         "Identifier for the voucher type linked to the customer account adjustment.",
         "",
         "",
         ""
        ],
        [
         "voucherpin",
         "✅",
         "Identifier for a voucher utilized in customer account adjustments, detailing voucher usage and its effects on the account.",
         "",
         "",
         ""
        ],
        [
         "additionalinfo",
         "✅",
         "Supplementary notes or extra details related to the customer account adjustment.",
         "",
         "",
         ""
        ],
        [
         "accounttypeid",
         "✅",
         "Accounttypeid specifies the category of the customer account for classification purposes.",
         "",
         "",
         ""
        ],
        [
         "accounttypename",
         "✅",
         "Categorization of the account type impacted by the adjustment.",
         "",
         "",
         ""
        ],
        [
         "subscriptiontypename",
         "✅",
         "Identifies the type of subscription linked to the customer account adjustment.",
         "",
         "",
         ""
        ],
        [
         "accessmethodnamekey",
         "✅",
         "Key identifier for access method used in adjustments to customer account.",
         "",
         "",
         ""
        ],
        [
         "status",
         "✅",
         "Indicates the current status of the customer account adjustment, reflecting if its processed, pending, approved, or declined.",
         "",
         "",
         ""
        ],
        [
         "externaltransactionreference",
         "✅",
         "Identifier for an external transaction linked to the customer account adjustment, aiding in external tracking and system integration.",
         "",
         "",
         ""
        ],
        [
         "postbalance",
         "✅",
         "Final account balance after adjustment, showing updated financial position.",
         "",
         "",
         ""
        ],
        [
         "subscriptionproperty",
         "✅",
         "Specification of subscription property for adjustment.",
         "",
         "",
         ""
        ],
        [
         "recordtags",
         "✅",
         "Additional tags for categorization and sorting of adjustment records",
         "",
         "",
         ""
        ],
        [
         "txn_date",
         "✅",
         "txn_date partition",
         "",
         "",
         ""
        ],
        [
         "file_timestamp_bucket",
         "✅",
         "Represents the timestamp bucket derived from the current processing time, useful for grouping or partitioning data based on processing intervals",
         "",
         "",
         ""
        ],
        [
         "dbx_process_dttm",
         "✅",
         "Records the date and time when the data was processed in the DBX system, important for tracking data processing activities.",
         "",
         "",
         ""
        ],
        [
         "file_name",
         "✅",
         "Contains the name of the file from which the data was derived, aiding in data traceability and management",
         "",
         "",
         ""
        ],
        [
         "Comment",
         "❌",
         "",
         "Free-text comments or remarks provided by users or stakeholders regarding a specific record or transaction.",
         "",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "has_comment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "suggested_comment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tags_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tags_command",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    w = WorkspaceClient()\n",
    "    client = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "    yml_path = \".bundle/root/local/files/databricks.yml\"\n",
    "    etc_path = \".bundle/root/local/files/src/pldt/cu/etc/\"\n",
    "    tags_file_path = \"GDM_Databricks_Tag_List.csv\"\n",
    "    model_to_use = \"databricks-llama-4-maverick\"\n",
    "\n",
    "\n",
    "\n",
    "    with open(yml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    active_jobs = [line.strip() for line in lines if line.lstrip().startswith(\"- ./\") and not line.lstrip().startswith(\"# - ./\")]\n",
    "    filtered_jobs = []\n",
    "    for job in active_jobs:\n",
    "        if job.startswith(\"- ./\") and job.endswith(\".yml\"):\n",
    "            job_name = job.split(\"/\")[-1].replace(\".yml\", \"\")\n",
    "            job_name = job_name.replace(\"_job\", \"\")\n",
    "            filtered_jobs.append(job_name)\n",
    "\n",
    "    for job in filtered_jobs:\n",
    "        for file_path in glob.glob(f\"{etc_path}*{job}*\"):\n",
    "\n",
    "                print(\"#\"*80)\n",
    "                print(job.center(80))\n",
    "                print(\"#\"*80)\n",
    "\n",
    "                task_configuration = main_extractor(input_path=file_path, key=\"task_configuration\", feature=\"2\")\n",
    "                file_format = main_extractor(input_path=file_path, key=\"file_format\", feature=\"2\")\n",
    "                delimiter = main_extractor(input_path=file_path, key=\"delimiter\", feature=\"2\")\n",
    "                data_quality_configuration = main_extractor(input_path=file_path, key=\"data_quality_configuration\", feature=\"2\")\n",
    "                table_name = main_extractor(input_path=file_path, key=\"table_name\", feature=\"2\")\n",
    "                target_partition_list = main_extractor(input_path=file_path, key=\"target_partition_list\", feature=\"2\")\n",
    "                target_partition_list = [partition[\"partition_name\"] for partition in target_partition_list] if isinstance(target_partition_list, list) else []\n",
    "\n",
    "                schema_dict = extract_schema_from_check_schema_consistency(data_quality_configuration)\n",
    "                table_name_parts = table_name.split(\".\")\n",
    "                wde_index = next(i for i, part in enumerate(table_name_parts) if part.startswith(\"wde\"))\n",
    "                table_name_parts[wde_index] = table_name_parts[wde_index] + \"_dev\"\n",
    "                dev_table_name = \".\".join(table_name_parts)\n",
    "\n",
    "                dev_tbl_df = spark.table(dev_table_name)\n",
    "                schema_kv = {field.name: field.dataType.simpleString() for field in dev_tbl_df.schema.fields}\n",
    "                test_schema_matches(schema_dict, schema_kv)\n",
    "\n",
    "                # print(f\"-\"*60, \"CHECKING OF CONFIG PARTITION VS CATALOG TABLE PARTITION\", \"-\"*60)\n",
    "                print(f\"\")\n",
    "                table_partition = extract_partition_columns_from_create_table(dev_table_name)\n",
    "                for partition in target_partition_list:\n",
    "                    if partition in table_partition:\n",
    "                        print(f\"✅ Partition or Column '{partition}' found in table {dev_table_name}\")\n",
    "                    else:\n",
    "                        print(f\"❌ Partition or Column '{partition}' NOT found in table {dev_table_name}\")\n",
    "\n",
    "                # print(f\"-\"*50, \"CHECKING OF CONFIG RULES NEED TO APPLY DEPENDING ON THE FILE FORMAT\", \"-\"*50)\n",
    "                # print(f\"\")\n",
    "                if file_format.lower() == \"parquet\":\n",
    "                    rules_for_parquet(data_quality_configuration, file_format)\n",
    "                elif file_format.lower() == \"csv\" or file_format.lower() == \"txt\":\n",
    "                    rules_for_delimited_file(data_quality_configuration, file_format, delimiter)\n",
    "                else:\n",
    "                    print()\n",
    "\n",
    "\n",
    "                check_table_column_comments(table_name=dev_table_name, tags_file_path=tags_file_path, model_to_use=model_to_use)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UT-mate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}